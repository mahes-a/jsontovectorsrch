{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load  Files from Jira  into CogSearch \n",
    "### The issues from Jira are loaded into CogSearch by following below steps\n",
    "- Establish a connection with Jira using the Python SDK.\n",
    "- Retrieve the required issues from Jira using the JQL query and Search method.\n",
    "- Use Azure open ai to index the issue content.\n",
    "- Index the parsed chunks into Azure Cognitive Search.\n",
    "- Repeat the process for all the required files.\n",
    "\n",
    "#### Using the Azure Storage Pyhon SDK  to fetch the file stream and use PYPDF and Document Intelligence to chunk the page in memory and create a vector index\n",
    "- https://developer.atlassian.com/cloud/jira/platform/rest/v3/api-group-issue-search/#api-rest-api-3-search-post\n",
    "- Refer to https://github.com/MSUSAzureAccelerators/Azure-Cognitive-Search-Azure-OpenAI-Accelerator/blob/main/04-Complex-Docs.ipynb for loading large documents using PYPDF and Document Intelligence- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter,PdfReader \n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv  \n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import html\n",
    "from azure.storage.blob.aio import BlobClient\n",
    "from azure.storage.blob import ContainerClient\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "from azure.storage.blob import BlobClient\n",
    "import io  \n",
    "import requests\n",
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt \n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Configure environment variables  \n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header for cog search\n",
    "headers = {'Content-Type': 'application/json','api-key': os.getenv('AZURE_SEARCH_KEY')}\n",
    "\n",
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OpenAIEmbeddings(deployment=os.getenv(\"AZURE_OPENAI_EMBEDDEPLOY_NAME\"), chunk_size=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "\n",
    "# Generate Document Embeddings using OpenAI Ada 002\n",
    "# Read the text-sample.json\n",
    "with open('Elecitems.json', 'r', encoding='utf-8') as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generate_embeddings(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text, engine=\"text-embedding-ada-002\")\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "extracted_fields = []\n",
    "\n",
    "index_list=[]\n",
    "# Print the issues\n",
    "for item in input_data:\n",
    "    print(item)\n",
    "    content=''\n",
    "    content=json.dumps(item)\n",
    "    index_list.append({'id': item['shortitem'], 'title':item['itemDesc'], 'content':content })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build the issue list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "### Create Azure Search Vector-based Index\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.getenv('AZURE_SEARCH_KEY')}\n",
    "params = {'api-version': os.getenv('AZURE_SEARCH_API_VERSION')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create the Vector-based index in our Azure Search Engine where this content is going to land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_payload = {\n",
    "    \"name\": str_index_name,\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"filterable\": \"true\" },\n",
    "        {\"name\": \"title\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunk\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunkVector\",\"type\": \"Collection(Edm.Single)\",\"searchable\": \"true\",\"retrievable\": \"true\",\"dimensions\": 1536, \"vectorSearchProfile\": \"my-default-vector-profile\"},\n",
    "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"false\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"}\n",
    "        \n",
    "    ],\n",
    "     \"vectorSearch\": {\n",
    "        \"algorithms\": [\n",
    "            {\n",
    "                \"name\": \"my-hnsw-config-1\",\n",
    "                \"kind\": \"hnsw\",\n",
    "                \"hnswParameters\": {\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"profiles\": [\n",
    "            {\n",
    "                \"name\": \"my-default-vector-profile\",\n",
    "                \"algorithm\": \"my-hnsw-config-1\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "r = requests.put(os.getenv('AZURE_SEARCH_ENDPOINT') + \"/indexes/\" + str_index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push data to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for item in index_list:\n",
    "    print(\"Uploading chunks from\",item[\"id\"])\n",
    "    \n",
    "    try:\n",
    "        upload_payload = {\"value\": [\n",
    "                    {\n",
    "                        \"id\": item[\"id\"],\n",
    "                        \"title\": item[\"title\"],\n",
    "                        \"chunk\": item[\"content\"],\n",
    "                        \"chunkVector\": embedder.embed_query(item[\"content\"] if item[\"content\"]!=\"\" else \"-------\"),\n",
    "                        \"name\": item[\"id\"],\n",
    "                        \"location\": item[\"id\"],\n",
    "                        \"@search.action\": \"upload\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + str_index_name + \"/docs/index\",\n",
    "                                 data=json.dumps(upload_payload), headers=headers, params=params)\n",
    "        if r.status_code != 200:\n",
    "                print(r.status_code)\n",
    "                print(r.text)\n",
    "    except Exception as e:\n",
    "            print(\"Exception:\",e)\n",
    "            print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
